[user_conf_file]
conf_file1 = hadoop:/opt/hadoop-2.5.1/etc/hadoop/core-site.xml
conf_file2 = hadoop:/opt/hadoop-2.5.1/etc/hadoop/hdfs-site.xml
conf_file3 = hadoop:/opt/hadoop-2.5.1/etc/hadoop/yarn-site.xml
conf_file4 = hadoop:/opt/hadoop-2.5.1/etc/hadoop/mapred-site.xml
conf_file5 = hadoop:/opt/hadoop-2.5.1/etc/hadoop/capacity-scheduler.xml
conf_file6 = hbase:/opt/hbase-1.2.8/conf/hbase-site.xml

[cluster_hosts]
node1 = 192.168.127.11
node2 = 192.168.127.22
node3 = 192.168.127.33
node4 = 192.168.127.44


[core-site]
fs.defaultFS = hdfs://node1:8020/
hadoop.http.staticuser.user = dr.who
hadoop.tmp.dir = /tmp/hadoop-${user.name}
hadoop.security.authorization = false
hadoop.proxyuser.hive.groups = hive
hadoop.proxyuser.hive.hosts = node1
hadoop.proxyuser.hive.users = hive
hadoop.proxyuser.hue.hosts = *
hadoop.proxyuser.hue.groups = *
hadoop.proxyuser.hbase.hosts = *
hadoop.proxyuser.hbase.groups = *
hadoop.security.group.mapping = org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback


[hdfs-site]
dfs.replication = 3
dfs.namenode.name.dir = file:///disk1/hdfs/name
dfs.datanode.data.dir = file:///disk1/hdfs/data
dfs.namenode.checkpoint.dir = file:///disk1/hdfs/namesecondary
dfs.namenode.checkpoint.edits.dir = ${dfs.namenode.checkpoint.dir}
dfs.permissions.enabled = false
dfs.permissions.superusergroup = supergroup
dfs.webhdfs.enabled = true
dfs.namenode.http-address = 0.0.0.0:50070
dfs.namenode.secondary.https-address = 0.0.0.0:50091
dfs.namenode.secondary.http-address = 0.0.0.0:50090
dfs.support.append = true
dfs.client.read.shortcircuit = true
dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn._PORT
dfs.datanode.max.transfer.threads = 4096
dfs.datanode.failed.volumes.tolerated = 0


[yarn-site]
yarn.resourcemanager.hostname = node1
yarn.resourcemanager.scheduler.class = org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
yarn.nodemanager.aux-services = mapreduce_shuffle
yarn.nodemanager.aux-services.mapreduce_shuffle.class = org.apache.hadoop.mapred.ShuffleHandler
yarn.nodemanager.local-dirs = ${hadoop.tmp.dir}/nm-local-dir
yarn.nodemanager.localizer.cache.target-size-mb = 10240
yarn.nodemanager.localizer.fetch.thread-count = 4
yarn.nodemanager.localizer.client.thread-count = 5
yarn.resourcemanager.scheduler.address = ${yarn.resourcemanager.hostname}:8030
yarn.resourcemanager.resource-tracker.address = ${yarn.resourcemanager.hostname}:8031
yarn.resourcemanager.address = ${yarn.resourcemanager.hostname}:8032
yarn.resourcemanager.admin.address = ${yarn.resourcemanager.hostname}:8033
yarn.resourcemanager.webapp.address = ${yarn.resourcemanager.hostname}:8088
yarn.nodemanager.vmem-pmem-ratio = 2.1
yarn.nodemanager.resource.memory-mb = 2048
yarn.scheduler.minimum-allocation-mb = 128
yarn.scheduler.maximum-allocation-mb = 2048
yarn.nodemanager.resource.cpu-vcores = 2
yarn.scheduler.minimum-allocation-vcores = 1
yarn.scheduler.maximum-allocation-vcores = 2
yarn.nodemanager.pmem-check-enabled = true
yarn.nodemanager.vmem-check-enabled = true
yarn.am.liveness-monitor.expiry-interval-ms = 600000
yarn.nm.liveness-monitor.expiry-interval-ms = 600000
yarn.resourcemanager.am.max-attempts = 2
yarn.nodemanager.delete.debug-delay-sec = 0
yarn.resourcemanager.max-completed-applications = 10000
yarn.admin.acl = *
yarn.nodemanager.log-dirs = ${yarn.log.dir}/userlogs
yarn.log-aggregation-enable = false
yarn.log-aggregation.retain-seconds = -1
yarn.nodemanager.remote-app-log-dir = /tmp/logs
yarn.client.application-client-protocol.poll-interval-ms = 200
;yarn.resourcemanager.nodes.include-path
;yarn.resourcemanager.nodes.exclude-path
;yarn.web-proxy.address
;yarn.web-proxy.principal
;yarn.web-proxy.keytab



[mapred-site]
mapreduce.framework.name = yarn
mapreduce.jobhistory.address = 0.0.0.0:10020
mapreduce.jobhistory.webapp.address = 0.0.0.0:19888
mapreduce.jobhistory.done-dir = ${yarn.app.mapreduce.am.staging-dir}/history/done
mapreduce.jobhistory.intermediate-done-dir = ${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate
mapreduce.job.reduce.slowstart.completedmaps = 0.05
mapreduce.am.max-attempts = 2
mapreduce.job.ubertask.enable = false
mapred.child.java.opts = -Xmx1536m
yarn.app.mapreduce.am.resource.mb = 1536
yarn.app.mapreduce.am.resource.cpu-vcores = 1
yarn.app.mapreduce.am.command-opts = -Xmx1024m
yarn.app.mapreduce.am.staging-dir = /tmp/hadoop-yarn/staging
mapreduce.map.sort.spill.percent = 0.80
mapreduce.job.heap.memory-mb.ratio = 0.8
;yarn.app.mapreduce.am.job.recovery.enable = true
;mapreduce.map.memory.mb
;mapreduce.reduce.memory.mb
;mapreduce.map.java.opts
;mapreduce.reduce.java.opts



[capacity-scheduler]
yarn.scheduler.capacity.maximum-applications = 10000
yarn.scheduler.capacity.maximum-am-resource-percent = 0.1
yarn.scheduler.capacity.resource-calculator = org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator
yarn.scheduler.capacity.node-locality-delay = 40
yarn.scheduler.capacity.root.queues = default
yarn.scheduler.capacity.root.default.capacity = 100
yarn.scheduler.capacity.root.default.user-limit-factor = 1
yarn.scheduler.capacity.root.default.maximum-capacity =100
yarn.scheduler.capacity.root.default.state = RUNNING
yarn.scheduler.capacity.root.default.acl_submit_applications = *
yarn.scheduler.capacity.root.default.acl_administer_queue = *
;yarn.scheduler.capacity.<queue-path>.minimum-user-limit-percent



[hbase-site]
hbase.zookeeper.quorum = node1
hbase.zookeeper.property.clientPort = 2181
hbase.zookeeper.property.dataDir = /opt/zookeeper_datadir
zookeeper.session.timeout = 1200000
hbase.zookeeper.property.tickTime = 6000
hbase.rootdir = hdfs://node1:8020/hbase
hbase.cluster.distributed = true
hbase.tmp.dir = /tmp/hbase-${user.name}
hbase.master.port = 16000
hbase.master.info.port = 16010
hbase.regionserver.port = 16020
hbase.regionserver.info.port = 16030
hbase.regionserver.handler.count = 30
hbase.hregion.memstore.flush.size = 134217728
hbase.hstore.compactionThreshold = 3
hbase.hstore.compaction.max = 10
hbase.hregion.majorcompaction = 604800000
hbase.hregion.max.filesize = 10737418240
hbase.regionserver.region.split.policy = org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy
hfile.block.cache.size = 0.4
hbase.hregion.memstore.block.multiplier = 4
hbase.hstore.blockingStoreFiles = 10
hbase.regionserver.thrift.http = true
hbase.thrift.support.proxyuser = true
hbase.regionserver.thrift.framed = false
hbase.regionserver.thrift.compact = false
;hbase.thrift.security.qop = auth
;hbase.thrift.info.port =
;hbase.regionserver.thrift.port =














































